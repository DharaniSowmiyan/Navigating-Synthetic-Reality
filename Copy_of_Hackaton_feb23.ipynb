{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DharaniSowmiyan/Navigating-Synthetic-Reality/blob/main/Copy_of_Hackaton_feb23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub torch torchvision scikit-learn seaborn tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMX-ZZrVjk1P",
        "outputId": "4d113a53-d069-45d8-a35d-3d792680a11b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.25.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (26.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F83H3v17jErY",
        "outputId": "057d7380-45fc-4ff6-8e9e-41d79f6fba02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cifake-real-and-ai-generated-synthetic-images' dataset.\n",
            "Path to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "92CgxP8Ojn9v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xP0MuBIjq2T",
        "outputId": "0359cbca-9ee6-4c97-acbd-489637c07be5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
        "print(\"Dataset downloaded to:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B67OD5mj9MI",
        "outputId": "c5b78e9d-2e55-4029-827d-2b2576247a04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cifake-real-and-ai-generated-synthetic-images' dataset.\n",
            "Dataset downloaded to: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VszcQo-rj-Ou",
        "outputId": "270312cc-de26-4f07-f0af-a58e4560d4be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "transform_val_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
        "])"
      ],
      "metadata": {
        "id": "GfUEGeNnkCOB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Create FULL dataset with train transform\n",
        "full_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(path, \"train\"),\n",
        "    transform=transform_train  # â† Train aug for whole dataset\n",
        ")\n",
        "\n",
        "print(\"Classes:\", full_dataset.classes)\n",
        "print(\"Total images:\", len(full_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHEJ_UwkkFIb",
        "outputId": "9739a08c-0529-49d0-fa14-55d4eda2ea01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['FAKE', 'REAL']\n",
            "Total images: 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 3: NOW split (augmentation carries through)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# STEP 4: Override val_dataset transform (remove augmentation)\n",
        "val_dataset.dataset.transform = transform_val_test  # â† CRITICAL!\n",
        "\n",
        "# STEP 5: Create loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "SuWoeVaPtwd6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Validation size:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITO0R_JlkI4W",
        "outputId": "51683084-9286-4a4c-fb6e-20251478737a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 80000\n",
            "Validation size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "# Replace model.fc:\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),                           # â† ADD THIS\n",
        "    nn.Linear(512, 2)\n",
        ")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9QpUHeHkOFE",
        "outputId": "48b002dd-19a6-45aa-8fa6-7023f25a0a33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-4)  # Lower LR + L2"
      ],
      "metadata": {
        "id": "2E5zNbimkRt_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "mCdAsfb9kSYv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=20):  # Increased epochs\n",
        "    best_val_acc = 0\n",
        "    patience = 7\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        val_acc = evaluate(model, val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
        "        print(f\"Loss: {running_loss:.4f}\")\n",
        "        print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        # SCHEDULER STEP 1: Call after each epoch\n",
        "        scheduler.step()\n",
        "\n",
        "        # EARLY STOPPING\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")  # Save best\n",
        "            print(f\"New best model saved! Val Acc: {best_val_acc:.2f}%\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "# CREATE SCHEDULER BEFORE CALLING (after optimizer)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HaqWP7I_kXug"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na4_L19ukeDA",
        "outputId": "6b282c16-2e4d-4798-cbaf-19ec5f6d4612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 454/1250 [09:15<15:22,  1.16s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detailed_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=full_dataset.classes,\n",
        "                yticklabels=full_dataset.classes)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q-StMeb5kghN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detailed_metrics(model, val_loader)"
      ],
      "metadata": {
        "id": "0CYuFNRhkjfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to ADD after val_loader creation:\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(path, \"test\"),\n",
        "    transform=transform_val_test  # â† NO augmentation\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Test size: {len(test_dataset)}\")  # Expect: 20,000"
      ],
      "metadata": {
        "id": "ZBhzFQ24u_RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Replace val_loader with test_loader in evaluation:\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"True Test Accuracy: {test_acc:.2f}%\")\n",
        "detailed_metrics(model, test_loader)\n"
      ],
      "metadata": {
        "id": "kD4ku-Y_b1tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"resnet18_cifake_baseline.pth\")\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "id": "R_lmlPENklgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"resnet18_cifake_baseline.pth\")"
      ],
      "metadata": {
        "id": "Xeu0IEMzkz18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vUmIUfdM3HXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1) Helpers: Denormalize + Overlay\n",
        "# ----------------------------\n",
        "CIFAR_MEAN = [0.4914, 0.4822, 0.4465]\n",
        "CIFAR_STD  = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "def denormalize(img_tensor, mean=CIFAR_MEAN, std=CIFAR_STD):\n",
        "    \"\"\"\n",
        "    img_tensor: [3,H,W] normalized torch tensor\n",
        "    returns:    [H,W,3] numpy array in [0,1]\n",
        "    \"\"\"\n",
        "    img = img_tensor.detach().cpu().clone()\n",
        "    for c in range(3):\n",
        "        img[c] = img[c] * std[c] + mean[c]\n",
        "    img = img.clamp(0, 1)\n",
        "    return img.permute(1,2,0).numpy()"
      ],
      "metadata": {
        "id": "EPrexy2w3omj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# MEMBER B: Explainability Pack\n",
        "# Grad-CAM (fixed for 32x32) + Saliency + Saving Outputs\n",
        "# ============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Config\n",
        "# ----------------------------\n",
        "OUT_DIR = \"explainability_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "CIFAR_MEAN = [0.4914, 0.4822, 0.4465]\n",
        "CIFAR_STD  = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Denormalize + Overlay\n",
        "# ----------------------------\n",
        "def denormalize(img_tensor, mean=CIFAR_MEAN, std=CIFAR_STD):\n",
        "    \"\"\"\n",
        "    img_tensor: [3,H,W] normalized torch tensor\n",
        "    returns: [H,W,3] numpy in [0,1]\n",
        "    \"\"\"\n",
        "    img = img_tensor.detach().cpu().clone()\n",
        "    for c in range(3):\n",
        "        img[c] = img[c] * std[c] + mean[c]\n",
        "    img = img.clamp(0, 1)\n",
        "    return img.permute(1,2,0).numpy()\n",
        "\n",
        "def overlay_cam(img_hwc, cam_hw, alpha=0.35):\n",
        "    \"\"\"\n",
        "    img_hwc: [H,W,3] in [0,1]\n",
        "    cam_hw:  [H,W] in [0,1]\n",
        "    \"\"\"\n",
        "    heatmap = plt.cm.jet(cam_hw)[:, :, :3]\n",
        "    out = (1 - alpha) * img_hwc + alpha * heatmap\n",
        "    return np.clip(out, 0, 1)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Grad-CAM (for small 32x32)\n",
        "# Use layer2 or layer3 (NOT layer4)\n",
        "# ----------------------------\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        self.target_layer.register_forward_hook(self._forward_hook)\n",
        "        self.target_layer.register_full_backward_hook(self._backward_hook)\n",
        "\n",
        "    def _forward_hook(self, module, inp, out):\n",
        "        self.activations = out.detach()   # [B,C,h,w]\n",
        "\n",
        "    def _backward_hook(self, module, grad_in, grad_out):\n",
        "        self.gradients = grad_out[0].detach()  # [B,C,h,w]\n",
        "\n",
        "    def generate(self, x, class_idx=None):\n",
        "        \"\"\"\n",
        "        x: [1,3,H,W]\n",
        "        returns: cam_up [H,W] in [0,1], probs [2]\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        logits = self.model(x)\n",
        "\n",
        "        if class_idx is None:\n",
        "            class_idx = int(logits.argmax(dim=1).item())\n",
        "\n",
        "        self.model.zero_grad(set_to_none=True)\n",
        "        logits[0, class_idx].backward()\n",
        "\n",
        "        acts = self.activations\n",
        "        grads = self.gradients\n",
        "\n",
        "        # weights: GAP on gradients\n",
        "        weights = grads.mean(dim=(2,3), keepdim=True)  # [1,C,1,1]\n",
        "        cam = (weights * acts).sum(dim=1, keepdim=True)  # [1,1,h,w]\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # normalize\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-8)\n",
        "\n",
        "        # upsample to input size\n",
        "        cam_up = F.interpolate(cam, size=(x.shape[2], x.shape[3]),\n",
        "                               mode=\"bilinear\", align_corners=False)\n",
        "        cam_up = cam_up[0,0].detach().cpu().numpy()\n",
        "\n",
        "        probs = F.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "        return cam_up, probs\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Saliency Map (optional but good)\n",
        "# ----------------------------\n",
        "def saliency_map(model, x, class_idx=None):\n",
        "    \"\"\"\n",
        "    x: [1,3,H,W]\n",
        "    returns: saliency [H,W] in [0,1]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = x.clone().detach().requires_grad_(True)\n",
        "\n",
        "    logits = model(x)\n",
        "    if class_idx is None:\n",
        "        class_idx = int(logits.argmax(dim=1).item())\n",
        "\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    logits[0, class_idx].backward()\n",
        "\n",
        "    sal = x.grad.detach().abs().max(dim=1)[0][0]  # [H,W]\n",
        "    sal = sal - sal.min()\n",
        "    sal = sal / (sal.max() + 1e-8)\n",
        "    return sal.cpu().numpy()\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Show + Save Explainability Grid\n",
        "# Crisp display using interpolation=\"nearest\"\n",
        "# ----------------------------\n",
        "def show_and_save_explainability(\n",
        "    model, loader, class_names, device,\n",
        "    num_images=12,\n",
        "    target_layer_name=\"layer2\",   # layer2 gives more detailed CAM than layer3\n",
        "    save=True\n",
        "):\n",
        "    # choose layer\n",
        "    if target_layer_name == \"layer2\":\n",
        "        target_layer = model.layer2[-1]\n",
        "    else:\n",
        "        target_layer = model.layer3[-1]\n",
        "\n",
        "    cam_extractor = GradCAM(model, target_layer)\n",
        "\n",
        "    shown = 0\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            x = images[i:i+1]\n",
        "            y = int(labels[i].item())\n",
        "\n",
        "            # prediction\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                probs = F.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "                pred = int(np.argmax(probs))\n",
        "                conf = float(np.max(probs))\n",
        "\n",
        "            # grad-cam + saliency (for predicted class)\n",
        "            cam, _ = cam_extractor.generate(x, class_idx=pred)\n",
        "            sal = saliency_map(model, x, class_idx=pred)\n",
        "\n",
        "            # image + overlays\n",
        "            img = denormalize(images[i])\n",
        "            overlay = overlay_cam(img, cam, alpha=0.35)\n",
        "\n",
        "            # Plot (crisp pixels)\n",
        "            plt.figure(figsize=(12, 3))\n",
        "            plt.subplot(1,4,1)\n",
        "            plt.imshow(img, interpolation=\"nearest\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Original\\nTrue: {class_names[y]}\")\n",
        "\n",
        "            plt.subplot(1,4,2)\n",
        "            plt.imshow(cam, cmap=\"jet\", interpolation=\"nearest\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Grad-CAM ({target_layer_name})\")\n",
        "\n",
        "            plt.subplot(1,4,3)\n",
        "            plt.imshow(overlay, interpolation=\"nearest\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Overlay\\nPred: {class_names[pred]} ({conf:.2f})\")\n",
        "\n",
        "            plt.subplot(1,4,4)\n",
        "            plt.imshow(sal, cmap=\"gray\", interpolation=\"nearest\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(\"Saliency\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Save to folder (for report/demo)\n",
        "            if save:\n",
        "                fname = f\"{OUT_DIR}/img_{shown:03d}_true-{class_names[y]}_pred-{class_names[pred]}_{target_layer_name}.png\"\n",
        "                plt.figure(figsize=(12, 3))\n",
        "                plt.subplot(1,4,1); plt.imshow(img, interpolation=\"nearest\"); plt.axis(\"off\"); plt.title(f\"Original\\nTrue: {class_names[y]}\")\n",
        "                plt.subplot(1,4,2); plt.imshow(cam, cmap=\"jet\", interpolation=\"nearest\"); plt.axis(\"off\"); plt.title(f\"Grad-CAM ({target_layer_name})\")\n",
        "                plt.subplot(1,4,3); plt.imshow(overlay, interpolation=\"nearest\"); plt.axis(\"off\"); plt.title(f\"Overlay\\nPred: {class_names[pred]} ({conf:.2f})\")\n",
        "                plt.subplot(1,4,4); plt.imshow(sal, cmap=\"gray\", interpolation=\"nearest\"); plt.axis(\"off\"); plt.title(\"Saliency\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n",
        "                plt.close()\n",
        "\n",
        "            shown += 1\n",
        "            if shown >= num_images:\n",
        "                print(f\"Saved outputs to: {OUT_DIR}/ (if save=True)\")\n",
        "                return\n",
        "\n",
        "print(\"Class names:\", dataset.classes)  # ['FAKE', 'REAL']\n",
        "show_and_save_explainability(\n",
        "    model, val_loader, dataset.classes, device,\n",
        "    num_images=12,\n",
        "    target_layer_name=\"layer2\",   # try layer2 first\n",
        "    save=True\n",
        ")"
      ],
      "metadata": {
        "id": "V49t50AVBWUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchattacks\n",
        "import torchattacks\n",
        "from torchattacks import FGSM, PGD\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n"
      ],
      "metadata": {
        "id": "t39ge2mJT1FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this cell after model definition, BEFORE training\n",
        "model.load_state_dict(torch.load(\"resnet18_cifake_baseline.pth\"))\n",
        "model.eval()\n",
        "print(\"Loaded your saved model!\")\n"
      ],
      "metadata": {
        "id": "drQPE5-HWOCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use YOUR valloader (20k validation images)\n",
        "model.eval()\n",
        "batch = next(iter(val_loader))\n",
        "images, labels = batch[0][:32].to(device), batch[1][:32].to(device)  # 32 for stats\n",
        "\n",
        "# Filter FAKE images (label=1) with high confidence >0.9\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "    probs = F.softmax(outputs, dim=1)\n",
        "    fake_mask = (labels == 1) & (probs[:,1] > 0.9)  # FAKE class + high conf\n",
        "\n",
        "high_conf_fakes = images[fake_mask][:10]  # Top 10\n",
        "fake_labels = torch.ones(10, device=device)  # All label=1 (FAKE)\n",
        "\n",
        "print(f\"âœ… Found {high_conf_fakes.size(0)} high-confidence fake images\")\n",
        "print(\"Sample fake confidences:\", probs[fake_mask][:5,1].cpu().numpy())\n"
      ],
      "metadata": {
        "id": "nXcDsJ1xdQak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create attacks\n",
        "atk_fgsm = FGSM(model, eps=8/255)\n",
        "atk_pgd = PGD(model, eps=8/255, alpha=2/255, steps=20, random_start=True)\n",
        "\n",
        "# Generate adversarial examples\n",
        "adv_fgsm = atk_fgsm(high_conf_fakes, fake_labels.long())\n",
        "adv_pgd = atk_pgd(high_conf_fakes, fake_labels.long())\n",
        "\n",
        "print(\"âœ… Attacks generated!\")\n",
        "print(f\"Clean shape: {high_conf_fakes.shape}\")\n",
        "print(f\"FGSM shape: {adv_fgsm.shape}\")\n"
      ],
      "metadata": {
        "id": "GFC5wWVMeHMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_evasion(images, adv_images, labels, attack_name):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        clean_pred = F.softmax(model(images), dim=1)[:,1]  # Fake prob\n",
        "        adv_pred = F.softmax(model(adv_images), dim=1)[:,1]\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot([0,1], [clean_pred.mean().item(), adv_pred.mean().item()],\n",
        "             'o-', label=f'{attack_name} (Î”={adv_pred.mean()-clean_pred.mean():.3f})')\n",
        "    plt.axhline(0.5, color='r', linestyle='--', alpha=0.5)\n",
        "    plt.ylabel('Fake Confidence'); plt.xlabel('Attack')\n",
        "    plt.title(f'{attack_name} Evasion on High-Conf Fakes'); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_evasion(high_conf_fakes, adv_fgsm, fake_labels, 'FGSM')\n",
        "plot_evasion(high_conf_fakes, adv_pgd, fake_labels, 'PGD')\n",
        "\n",
        "# Success rate (fake â†’ real flip)\n",
        "fgsm_success = (F.softmax(model(adv_fgsm),1)[:,0] > 0.5).float().mean()\n",
        "pgd_success = (F.softmax(model(adv_pgd),1)[:,0] > 0.5).float().mean()\n",
        "print(f\"âœ… FGSM Success: {fgsm_success:.1%} | PGD Success: {pgd_success:.1%}\")\n"
      ],
      "metadata": {
        "id": "N8YEBmTeeQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import CW\n",
        "atk_cw = CW(model, c=10, lr=0.01, steps=50)\n",
        "adv_cw = atk_cw(high_conf_fakes, fake_labels.long())\n",
        "print(\"âœ… C&W L2: Unconstrained optimization attack\")\n"
      ],
      "metadata": {
        "id": "JbMDVrAekExe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import DeepFool\n",
        "atk_deepfool = DeepFool(model, steps=50)\n",
        "adv_deepfool = atk_deepfool(high_conf_fakes, fake_labels.long())\n",
        "print(\"âœ… DeepFool: Finds smallest perturbation to fool\")"
      ],
      "metadata": {
        "id": "68Ebd-2qkItI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import OnePixel\n",
        "atk_onepixel = OnePixel(model)\n",
        "adv_onepixel = atk_onepixel(high_conf_fakes, fake_labels.long())\n",
        "print(\"âœ… OnePixel: Changes ONLY 1 pixel per image!\")\n"
      ],
      "metadata": {
        "id": "ykWxqP4HkPMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import Square\n",
        "atk_square = Square(model, eps=8/255)\n",
        "adv_square = atk_square(high_conf_fakes, fake_labels.long())\n",
        "print(\"âœ… Square: Realistic adversarial patch attack\")\n"
      ],
      "metadata": {
        "id": "fmDo5pCZkTYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attacks = {\n",
        "    'Clean': high_conf_fakes,\n",
        "    'FGSM': adv_fgsm,\n",
        "    'PGD': adv_pgd,\n",
        "    'DeepFool': adv_deepfool,\n",
        "    'C&W': adv_cw,\n",
        "    'OnePixel': adv_onepixel,\n",
        "    'Square': adv_square\n",
        "}\n",
        "\n",
        "results = {}\n",
        "model.eval()\n",
        "for name, adv_imgs in attacks.items():\n",
        "    with torch.no_grad():\n",
        "        fake_conf = F.softmax(model(adv_imgs), dim=1)[:,1].mean().item()\n",
        "        success = (F.softmax(model(adv_imgs),1)[:,0] > 0.5).float().mean().item()\n",
        "    results[name] = {'fake_conf': fake_conf, 'success': success}\n",
        "    print(f\"{name:10} | Fake: {fake_conf:.1%} | Success: {success:.1%}\")\n",
        "\n",
        "# Table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results).T\n",
        "print(\"\\nðŸ† FULL ATTACK BENCHMARK:\")\n",
        "print(df.round(3))\n"
      ],
      "metadata": {
        "id": "zd3RxvUEkos0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DYNAMIC PLOT - Works with ANY number of attacks automatically\n",
        "model.eval()\n",
        "\n",
        "# Your existing attacks dict (add all your adv_* variables here)\n",
        "attack_dict = {\n",
        "    'Clean': high_conf_fakes,\n",
        "    'FGSM': adv_fgsm,\n",
        "    'PGD': adv_pgd,\n",
        "    'DeepFool': adv_deepfool,\n",
        "    'C&W': adv_cw,\n",
        "    'OnePixel': adv_onepixel,\n",
        "    'Square': adv_square\n",
        "    # Add more: 'PGD_strong': adv_pgd_strong, etc.\n",
        "}\n",
        "\n",
        "# Auto-compute metrics\n",
        "metrics = {}\n",
        "for name, adv_imgs in attack_dict.items():\n",
        "    with torch.no_grad():\n",
        "        # Get raw probabilities for the batch\n",
        "        probs = F.softmax(model(adv_imgs), dim=1)\n",
        "\n",
        "        fake_conf = probs[:,1].mean().item()\n",
        "        # Calculate success rate: proportion of images where Real probability > 0.5\n",
        "        success = (probs[:,0] > 0.5).float().mean().item()\n",
        "    metrics[name] = {'fake_conf': fake_conf, 'success': success}\n",
        "\n",
        "# ðŸ”¥ DYNAMIC PLOT 1: Confidence DROP\n",
        "plt.figure(figsize=(12, 4))\n",
        "names = list(metrics.keys())\n",
        "fake_confs = [metrics[n]['fake_conf'] for n in names]\n",
        "success_rates = [metrics[n]['success'] for n in names]\n",
        "\n",
        "x = np.arange(len(names))\n",
        "width = 0.35\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "bars1 = plt.bar(x - width/2, fake_confs, width, label='Fake Conf', alpha=0.8, color='red')\n",
        "plt.bar(x + width/2, success_rates, width, label='Attack Success', alpha=0.8, color='green')\n",
        "plt.axhline(0.5, color='black', linestyle='--', alpha=0.7, label='Decision Boundary')\n",
        "plt.xlabel('Attacks'); plt.ylabel('Probability'); plt.title('All Attacks: Fake Conf vs Success')\n",
        "plt.xticks(x, [n[:8] for n in names], rotation=45)\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "# ðŸ”¥ DYNAMIC PLOT 2: Attack Effectiveness Radar\n",
        "plt.subplot(1, 2, 2, projection='polar') # Moved projection here\n",
        "angles = np.linspace(0, 2*np.pi, len(names), endpoint=False).tolist()\n",
        "\n",
        "fake_drop = [(1-metrics[n]['fake_conf']) for n in names]  # How much fake conf dropped\n",
        "success_norm = [metrics[n]['success'] for n in names]\n",
        "\n",
        "# Combine fake_drop and success_norm element-wise into a single effectiveness score per attack\n",
        "combined_effectiveness = [(fd + sn) / 2 for fd, sn in zip(fake_drop, success_norm)]\n",
        "\n",
        "values = np.array(combined_effectiveness)\n",
        "values = np.append(values, values[0])  # Close circle\n",
        "angles += angles[:1] # Close circle for angles too, after calculating values\n",
        "\n",
        "ax = plt.gca() # Get current axes after subplot setup\n",
        "ax.plot(angles, values, 'o-', linewidth=2)\n",
        "ax.fill(angles, values, alpha=0.25)\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels([n[:10] for n in names])\n",
        "ax.set_ylim(0, 1)\n",
        "plt.title('Attack Effectiveness (Radar)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ðŸ“Š DYNAMIC TABLE (auto-formatted)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(metrics).T.round(3)\n",
        "df['Î”Fake'] = (1 - df['fake_conf']).round(3)\n",
        "df = df[['fake_conf', 'Î”Fake', 'success']]\n",
        "print(\"\\nðŸ† DYNAMIC ATTACK COMPARISON:\")\n",
        "print(df.style.background_gradient(cmap='RdYlGn_r', subset=['success']).format('{:.1%}'))"
      ],
      "metadata": {
        "id": "gGmzeb7Ul3Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¥ GRAD-CAM: Clean vs ALL Attacks (5 images x 2 rows)\n",
        "def plot_all_gradcam(gradcam_extractor, attacks_dict, num_images=5):\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate total columns required for num_images per attack type\n",
        "    total_cols = len(attacks_dict) * num_images\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(2, total_cols,\n",
        "                           figsize=(total_cols * 3, 8))\n",
        "\n",
        "\n",
        "    if total_cols == 1:\n",
        "        axes = axes.reshape(2, 1)\n",
        "\n",
        "\n",
        "    for col_idx_for_attack, (attack_name, adv_imgs) in enumerate(attacks_dict.items()):\n",
        "        for row, imgs in enumerate([high_conf_fakes, adv_imgs]):\n",
        "            title_prefix = 'Clean (Fake)' if row == 0 else attack_name\n",
        "\n",
        "            for i in range(num_images):\n",
        "                # Calculate the actual column index in the subplot grid\n",
        "                current_plot_col = (col_idx_for_attack * num_images) + i\n",
        "\n",
        "                x = imgs[i:i+1] # Get single image batch\n",
        "                y_true = int(fake_labels[i].item()) # True label for this image (always 1 for FAKE)\n",
        "\n",
        "                # Prediction on the image (clean or adversarial)\n",
        "                with torch.no_grad():\n",
        "                    logits = model(x)\n",
        "                    probs = F.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "                    pred = int(np.argmax(probs))\n",
        "                    conf = float(np.max(probs))\n",
        "\n",
        "                # Grad-CAM generation for the *predicted* class\n",
        "                cam, _ = gradcam_extractor.generate(x, class_idx=pred)\n",
        "\n",
        "                # Denormalize image for display and overlay CAM\n",
        "                original_img_hwc = denormalize(x[0])\n",
        "                cam_overlayed = overlay_cam(original_img_hwc, cam, alpha=0.6)\n",
        "\n",
        "                # Get the specific subplot axis\n",
        "                ax = axes[row, current_plot_col]\n",
        "\n",
        "                # Plot the overlaid image\n",
        "                ax.imshow(cam_overlayed, interpolation=\"nearest\")\n",
        "                ax.set_title(f'{title_prefix}\\nTrue:{dataset.classes[y_true]}\\nPred:{dataset.classes[pred]} ({conf:.2f})', fontsize=8)\n",
        "                ax.axis('off')\n",
        "\n",
        "    plt.suptitle('Grad-CAM: Clean (artifacts, red) â†’ Attacks (noise shift)', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Use YOUR attacks (top 4 strongest)\n",
        "top_attacks = {\n",
        "    'DeepFool': adv_deepfool,  # 60% success!\n",
        "    'PGD': adv_pgd,\n",
        "    'C&W': adv_cw,\n",
        "    'FGSM': adv_fgsm\n",
        "}\n",
        "\n",
        "# Initialize GradCAM extractor\n",
        "target_layer = model.layer2[-1] # Using 'layer2' as in the previous example\n",
        "gradcam_extractor = GradCAM(model, target_layer)\n",
        "\n",
        "plot_all_gradcam(gradcam_extractor, top_attacks, num_images=3) # 3 images per attack"
      ],
      "metadata": {
        "id": "be5a-Nv9nI-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "batch = next(iter(val_loader)) # Using val_loader here as it was originally intended\n",
        "images, labels = batch[0][:32].to(device), batch[1][:32].to(device)  # 32 for stats\n",
        "\n",
        "# Filter FAKE images (label=0) with high confidence >0.9\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "    probs = F.softmax(outputs, dim=1)\n",
        "    fake_mask = (labels == 0) & (probs[:,0] > 0.9)  # FAKE class (0) + high conf (prob of 0 > 0.9)\n",
        "\n",
        "high_conf_fakes = images[fake_mask][:10]  # Top 10\n",
        "fake_labels = torch.zeros(high_conf_fakes.size(0), device=device, dtype=torch.long)  # All label=0 (FAKE) - Ensure long dtype\n",
        "\n",
        "print(f\"âœ… Found {high_conf_fakes.size(0)} high-confidence fake images\")\n",
        "print(\"Sample fake confidences:\", probs[fake_mask][:5,0].cpu().numpy())"
      ],
      "metadata": {
        "id": "s-paDVletyEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check raw test predictions and collect high-confidence FAKE images\n",
        "all_images = []\n",
        "all_true_labels = [] # Actual labels from dataset (0 for FAKE, 1 for REAL)\n",
        "all_predicted_fake_probs = [] # Model's predicted probability of being FAKE (class 0)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images_on_device = images.to(device)\n",
        "        outputs = model(images_on_device)\n",
        "        probs = F.softmax(outputs, 1) # [:,0] for FAKE, [:,1] for REAL\n",
        "\n",
        "        all_images.append(images)\n",
        "        all_true_labels.extend(labels.cpu().numpy())\n",
        "        all_predicted_fake_probs.extend(probs[:,0].cpu().numpy()) # Probability of being FAKE (class 0)\n",
        "\n",
        "# Combine collected tensors for easier filtering\n",
        "all_images_tensor = torch.cat(all_images, dim=0)\n",
        "all_true_labels_tensor = torch.tensor(all_true_labels)\n",
        "all_predicted_fake_probs_tensor = torch.tensor(all_predicted_fake_probs)\n",
        "\n",
        "# 2. Identify high-confidence FAKE images (true label is FAKE (0) and model is confident it's FAKE (>0.9))\n",
        "fake_mask_high_conf = (all_true_labels_tensor == 0) & (all_predicted_fake_probs_tensor > 0.9)\n",
        "\n",
        "test_high_conf = all_images_tensor[fake_mask_high_conf].to(device)\n",
        "test_high_conf_labels = all_true_labels_tensor[fake_mask_high_conf].to(device) # These labels should all be 0\n",
        "\n",
        "print(f\"Fake image max confidence (true FAKE images): {all_predicted_fake_probs_tensor[all_true_labels_tensor == 0].max():.3f}\")\n",
        "print(f\"True FAKE images with model confidence > 0.7: {sum(all_predicted_fake_probs_tensor[all_true_labels_tensor == 0] > 0.7)}\")\n",
        "print(f\"True FAKE images with model confidence > 0.9: {sum(all_predicted_fake_probs_tensor[all_true_labels_tensor == 0] > 0.9)}\")\n",
        "print(f\"\\nâœ… Found {test_high_conf.size(0)} high-confidence true FAKE images for adversarial testing.\")\n",
        "print(\"Sample fake confidences from these selected images:\", all_predicted_fake_probs_tensor[fake_mask_high_conf][:5].cpu().numpy())"
      ],
      "metadata": {
        "id": "HRV72reKuR1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAME ATTACKS but on YOUR test_high_conf (263 fakes)\n",
        "atk_fgsm_test = FGSM(model, eps=12/255)\n",
        "atk_pgd_test = PGD(model, eps=12/255, alpha=3/255, steps=40, random_start=True)\n",
        "atk_deepfool_test = DeepFool(model, steps=100) # eps removed\n",
        "atk_cw_test = CW(model, c=20, lr=0.02, steps=100)\n",
        "\n",
        "# Attack first 50 for speed (or all 263 if time)\n",
        "# Make sure test_high_conf has enough images, otherwise take all available\n",
        "num_samples_to_attack = min(50, test_high_conf.size(0))\n",
        "test_sample = test_high_conf[:num_samples_to_attack]\n",
        "test_sample_labels = test_high_conf_labels[:num_samples_to_attack] # Use the actual labels (which are 0 for FAKE)\n",
        "\n",
        "adv_fgsm_test = atk_fgsm_test(test_sample, test_sample_labels)\n",
        "adv_pgd_test = atk_pgd_test(test_sample, test_sample_labels)\n",
        "adv_deepfool_test = atk_deepfool_test(test_sample, test_sample_labels)\n",
        "adv_cw_test = atk_cw_test(test_sample, test_sample_labels)\n",
        "\n",
        "print(\"âœ… Test loader attacks COMPLETE!\")"
      ],
      "metadata": {
        "id": "v97_22eKwSO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR test attacks dict\n",
        "test_attacks = {\n",
        "    'Clean': test_sample,\n",
        "    'FGSM': adv_fgsm_test,\n",
        "    'PGD': adv_pgd_test,\n",
        "    'DeepFool': adv_deepfool_test,\n",
        "    'C&W': adv_cw_test\n",
        "}\n",
        "\n",
        "test_results = {}\n",
        "model.eval()\n",
        "for name, imgs in test_attacks.items():\n",
        "    success = (F.softmax(model(imgs),1)[:,0] > 0.5).float().mean().item()\n",
        "    test_results[name] = success\n",
        "    print(f\"{name:10} | Success: {success:.1%} (TEST SET)\")\n",
        "\n",
        "print(f\"\\nðŸ”¥ TEST SET BENCHMARK (n={len(test_sample)})\")\n"
      ],
      "metadata": {
        "id": "nENxXtdZwt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set Grad-CAM (BEST attack only - DeepFool)\n",
        "def test_gradcam_comparison(gradcam_extractor):\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    for i in range(4):\n",
        "        # Test Clean (originally FAKE, class 0)\n",
        "        cam_clean, _ = gradcam_extractor.generate(test_sample[i:i+1], class_idx=0)\n",
        "        axes[0,i].imshow(cam_clean, cmap='jet'); axes[0,i].set_title('Test Clean (Pred: FAKE)')\n",
        "        axes[0,i].axis('off')\n",
        "\n",
        "        # Test DeepFool attack (fooled to REAL, class 1)\n",
        "        cam_adv, _ = gradcam_extractor.generate(adv_deepfool_test[i:i+1], class_idx=1)\n",
        "        axes[1,i].imshow(cam_adv, cmap='jet'); axes[1,i].set_title('Test DeepFool (Pred: REAL)')\n",
        "        axes[1,i].axis('off')\n",
        "\n",
        "    plt.suptitle('TEST SET: Clean (artifacts) \\u2192 DeepFool (noise)', fontsize=14)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "test_gradcam_comparison(gradcam_extractor)"
      ],
      "metadata": {
        "id": "ipkVUy3qwzCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fft\n",
        "\n",
        "def high_pass_defense(img_tensor, cutoff=0.1):\n",
        "    \"\"\"\n",
        "    Remove low-frequency adversarial noise while keeping high-freq artifacts\n",
        "    cutoff=0.1 blocks ~10% lowest frequencies (FGSM sweet spot)\n",
        "    \"\"\"\n",
        "    # Normalize to [0,1] for FFT\n",
        "    img = (img_tensor - img_tensor.min()) / (img_tensor.max() - img_tensor.min() + 1e-8)\n",
        "\n",
        "    # FFT â†’ Zero low frequencies â†’ IFFT\n",
        "    fft = torch.fft.fft2(img)\n",
        "    h, w = fft.shape[-2:]\n",
        "\n",
        "    # Create high-pass mask (block low freq center)\n",
        "    center_h, center_w = h//2, w//2\n",
        "    mask = torch.ones_like(fft)\n",
        "    # Corrected indexing for 3D tensor (C, H, W)\n",
        "    mask[:, center_h-int(cutoff*h):center_h+int(cutoff*h),\n",
        "          center_w-int(cutoff*w):center_w+int(cutoff*w)] = 0\n",
        "\n",
        "    # Apply filter + inverse FFT\n",
        "    filtered_fft = fft * mask\n",
        "    filtered = torch.fft.ifft2(filtered_fft).real\n",
        "\n",
        "    # Denormalize back\n",
        "    return filtered * (img_tensor.max() - img_tensor.min()) + img_tensor.min()\n",
        "\n",
        "# Test on YOUR FGSM attack (34% success)\n",
        "model.eval()\n",
        "print(\"=== FREQUENCY DEFENSE TEST ===\")\n",
        "\n",
        "# Original FGSM attack (34% success)\n",
        "fgsm_success_before = (F.softmax(model(adv_fgsm_test),1)[:,0] > 0.5).float().mean()\n",
        "print(f\"FGSM Baseline: {fgsm_success_before:.1%}\")\n",
        "\n",
        "# Apply defense iteratively (fix: removed [None] to avoid extra dimension)\n",
        "filtered_fgsm = torch.stack([high_pass_defense(img, cutoff=0.15) for img in adv_fgsm_test])\n",
        "fgsm_success_after = (F.softmax(model(filtered_fgsm.to(device)),1)[:,0] > 0.5).float().mean()\n",
        "print(f\"FGSM + High-Pass: {fgsm_success_after:.1%}\")\n",
        "print(f\"ðŸ›¡ï¸ ROBUSTNESS GAIN: {fgsm_success_before - fgsm_success_after:.1%}\")\n",
        "\n",
        "# Visual proof\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "img_idx = 0\n",
        "\n",
        "# Original clean â†’ attack â†’ defended\n",
        "axes[0,0].imshow(test_sample[img_idx].cpu().permute(1,2,0))\n",
        "axes[0,0].set_title('Clean\\n100% Fake')\n",
        "axes[0,1].imshow(adv_fgsm_test[img_idx].cpu().permute(1,2,0))\n",
        "axes[0,1].set_title('FGSM Attack\\n66% Real')\n",
        "axes[0,2].imshow(filtered_fgsm[img_idx].cpu().permute(1,2,0)) # Changed filtered_fgsm[0] to filtered_fgsm[img_idx]\n",
        "axes[0,2].set_title(f'Defended\\n{fgsm_success_after:.0%} Attack')\n",
        "\n",
        "# Grad-CAM proof (fix: use gradcam_extractor and remove [0,0] indexing)\n",
        "cam_orig, _ = gradcam_extractor.generate(test_sample[img_idx:img_idx+1], class_idx=0) # FAKE is 0\n",
        "axes[1,0].imshow(cam_orig, cmap='jet')\n",
        "axes[1,0].set_title('Clean CAM')\n",
        "\n",
        "cam_attack, _ = gradcam_extractor.generate(adv_fgsm_test[img_idx:img_idx+1], class_idx=1) # REAL is 1\n",
        "axes[1,1].imshow(cam_attack, cmap='jet')\n",
        "axes[1,1].set_title('Attack CAM')\n",
        "\n",
        "cam_defended, _ = gradcam_extractor.generate(filtered_fgsm[img_idx:img_idx+1].to(device), class_idx=0) # FAKE is 0\n",
        "axes[1,2].imshow(cam_defended, cmap='jet')\n",
        "axes[1,2].set_title('Defended CAM')\n",
        "\n",
        "plt.suptitle('High-Pass Defense: Recovers Artifact Focus', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zi2rag4ZyGMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ON your FGSM attacks (most successful)\n",
        "adv_train_dataset = TensorDataset(adv_fgsm_test, test_sample_labels)\n",
        "adv_loader = DataLoader(adv_train_dataset, 32, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "for epoch in range(5):  # Quick\n",
        "    for adv_imgs, lbls in adv_loader:\n",
        "        out = model(adv_imgs.to(device))\n",
        "        loss = criterion(out, lbls.to(device))\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "# Re-test FGSM on robust model\n",
        "robust_fgsm_success = (F.softmax(model(adv_fgsm_test),1)[:,0] > 0.5).float().mean()\n",
        "print(f\"FGSM: 34% â†’ {robust_fgsm_success:.1%}\")\n"
      ],
      "metadata": {
        "id": "yOyEvkHs01G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 3 models + vote\n",
        "models = [model]  # Add 2 more ResNets if time\n",
        "predictions = []\n",
        "for m in models:\n",
        "    pred = F.softmax(m(adv_fgsm_test),1)[:,0] > 0.5\n",
        "    predictions.append(pred.float())\n",
        "\n",
        "ensemble_pred = torch.stack(predictions).mean(0) > 0.5\n",
        "ensemble_success = ensemble_pred.float().mean()\n",
        "print(f\"Ensemble FGSM: {ensemble_success:.1%}\")\n"
      ],
      "metadata": {
        "id": "wRYdOoIW07yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# COMBINE your best attacks for training\n",
        "all_adv_attacks = [\n",
        "    (adv_deepfool.cpu(), fake_labels.cpu()),      # Val DeepFool (60%) - ensure all tensors are on CPU before cat\n",
        "    (adv_fgsm_test.cpu(), test_sample_labels.cpu()) # Test FGSM (34%)\n",
        "]\n",
        "\n",
        "# Create training dataset from attacks\n",
        "all_images_for_training = []\n",
        "all_labels_for_training = []\n",
        "\n",
        "for adv_imgs, lbls in all_adv_attacks:\n",
        "    all_images_for_training.append(adv_imgs)\n",
        "    all_labels_for_training.append(lbls)\n",
        "\n",
        "# Concatenate all image tensors and label tensors into single tensors\n",
        "adv_train_images = torch.cat(all_images_for_training, dim=0)\n",
        "adv_train_labels = torch.cat(all_labels_for_training, dim=0)\n",
        "\n",
        "adv_dataset = TensorDataset(adv_train_images, adv_train_labels)\n",
        "adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Quick adversarial fine-tuning (5 epochs)\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"ðŸ›¡ï¸ ADVERSARIAL TRAINING (DeepFool + FGSM)...\")\n",
        "for epoch in range(5):\n",
        "    epoch_loss = 0\n",
        "    for adv_imgs, labels in adv_loader:\n",
        "        adv_imgs, labels = adv_imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(adv_imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/5, Loss: {epoch_loss/len(adv_loader):.3f}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'cifake_robust_model.pth')\n",
        "print(\"âœ… Robust model SAVED!\")\n",
        "\n",
        "# Test improvement\n",
        "model.eval()\n",
        "deepfool_success_after = (F.softmax(model(adv_deepfool),1)[:,0] > 0.5).float().mean()\n",
        "fgsm_success_after = (F.softmax(model(adv_fgsm_test),1)[:,0] > 0.5).float().mean()\n",
        "\n",
        "print(f\"\\nðŸ† ROBUSTNESS RESULTS:\")\n",
        "print(f\"DeepFool: 60% â†’ {deepfool_success_after:.1%}\")\n",
        "print(f\"FGSM:    34% â†’ {fgsm_success_after:.1%}\")"
      ],
      "metadata": {
        "id": "kdf7v8AM1wrb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}